{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:25:37.364440Z",
     "iopub.status.busy": "2026-01-07T08:25:37.363652Z",
     "iopub.status.idle": "2026-01-07T08:25:49.943755Z",
     "shell.execute_reply": "2026-01-07T08:25:49.942689Z",
     "shell.execute_reply.started": "2026-01-07T08:25:37.364385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hüöÄ Fetching reviews for in.swiggy.android from 2026-01-05 to 2026-01-07...\n",
      "\n",
      "‚úÖ Ingestion Complete!\n",
      "üìÇ Saved to: /kaggle/working/in.swiggy.android_raw_reviews.csv\n",
      "üìä Total Reviews: 447\n",
      "üî¢ Total Token Count: ~5167 tokens\n",
      "------------------------------\n",
      "üìù Sample Reviews (First 10):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-06 08:24:47</td>\n",
       "      <td>worst app don't buy any thing from Instamart.....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-06 08:19:23</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-06 08:17:28</td>\n",
       "      <td>Best</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-06 08:10:32</td>\n",
       "      <td>login problem too much</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-06 08:10:25</td>\n",
       "      <td>good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-06 08:07:26</td>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-06 08:05:37</td>\n",
       "      <td>good app</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-06 08:02:52</td>\n",
       "      <td>Worst technical team and support team. Was one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-06 08:01:45</td>\n",
       "      <td>food taste very good üòã</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-06 07:59:19</td>\n",
       "      <td>total froud worst app.full itam not delivered</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date                                            content  \\\n",
       "0 2026-01-06 08:24:47  worst app don't buy any thing from Instamart.....   \n",
       "1 2026-01-06 08:19:23                                               good   \n",
       "2 2026-01-06 08:17:28                                               Best   \n",
       "3 2026-01-06 08:10:32                             login problem too much   \n",
       "4 2026-01-06 08:10:25                                               good   \n",
       "5 2026-01-06 08:07:26                                               good   \n",
       "6 2026-01-06 08:05:37                                           good app   \n",
       "7 2026-01-06 08:02:52  Worst technical team and support team. Was one...   \n",
       "8 2026-01-06 08:01:45                             food taste very good üòã   \n",
       "9 2026-01-06 07:59:19      total froud worst app.full itam not delivered   \n",
       "\n",
       "   score  \n",
       "0      1  \n",
       "1      5  \n",
       "2      5  \n",
       "3      1  \n",
       "4      3  \n",
       "5      5  \n",
       "6      5  \n",
       "7      1  \n",
       "8      5  \n",
       "9      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1: Fetch, Store, and Calculate Tokens\n",
    "!pip install -q google-play-scraper pandas google-generativeai\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "from google_play_scraper import Sort, reviews\n",
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "APP_ID = 'in.swiggy.android' \n",
    "# APP_ID = 'com.application.zomato' # Change this to fetch different apps\n",
    "DAYS_TO_FETCH = 2\n",
    "WORKING_DIR = \"/kaggle/working\"\n",
    "\n",
    "# Setup Dates\n",
    "END_DATE = datetime.datetime.now()\n",
    "START_DATE = END_DATE - datetime.timedelta(days=DAYS_TO_FETCH)\n",
    "\n",
    "# --- 2. Ingestion Engine ---\n",
    "print(f\"üöÄ Fetching reviews for {APP_ID} from {START_DATE.date()} to {END_DATE.date()}...\")\n",
    "\n",
    "all_reviews = []\n",
    "continuation_token = None\n",
    "\n",
    "# We fetch a bit more to ensure coverage, then filter\n",
    "while True:\n",
    "    result, continuation_token = reviews(\n",
    "        APP_ID,\n",
    "        lang='en',\n",
    "        country='in',\n",
    "        sort=Sort.NEWEST,\n",
    "        count=200, \n",
    "        continuation_token=continuation_token\n",
    "    )\n",
    "    \n",
    "    if not result: break\n",
    "    \n",
    "    # Check if we have passed the start date\n",
    "    oldest_in_batch = result[-1]['at']\n",
    "    \n",
    "    for r in result:\n",
    "        if r['at'] >= START_DATE:\n",
    "            all_reviews.append({\n",
    "                'id': r['reviewId'],\n",
    "                'date': r['at'],\n",
    "                'content': r['content'],\n",
    "                'score': r['score'],\n",
    "                'app_id': APP_ID\n",
    "            })\n",
    "    \n",
    "    if oldest_in_batch < START_DATE:\n",
    "        break\n",
    "\n",
    "# --- 3. Data Storage (Separated by App ID) ---\n",
    "df = pd.DataFrame(all_reviews)\n",
    "csv_filename = f\"{WORKING_DIR}/{APP_ID}_raw_reviews.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# --- 4. Token Counting & Sampling ---\n",
    "# Initialize API just for token counting (no cost)\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    GOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "    \n",
    "    # Prepare text payload for counting\n",
    "    full_text_payload = \"\\n\".join(df['content'].tolist())\n",
    "    token_count = model.count_tokens(full_text_payload).total_tokens\n",
    "    \n",
    "except Exception as e:\n",
    "    token_count = len(full_text_payload) // 4 # Rough fallback\n",
    "    print(f\"‚ö†Ô∏è API Key Error (Using estimate): {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ingestion Complete!\")\n",
    "print(f\"üìÇ Saved to: {csv_filename}\")\n",
    "print(f\"üìä Total Reviews: {len(df)}\")\n",
    "print(f\"üî¢ Total Token Count: ~{token_count} tokens\")\n",
    "print(\"-\" * 30)\n",
    "print(\"üìù Sample Reviews (First 10):\")\n",
    "display(df[['date', 'content', 'score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:33:07.212101Z",
     "iopub.status.busy": "2026-01-07T08:33:07.211786Z",
     "iopub.status.idle": "2026-01-07T08:33:37.753473Z",
     "shell.execute_reply": "2026-01-07T08:33:37.752561Z",
     "shell.execute_reply.started": "2026-01-07T08:33:07.212074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Calling Gemini 2.5 Pro for Strategic Analysis...\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Of course. Here is the Strategic Trend Analysis Report.\n",
       "\n",
       "***\n",
       "\n",
       "### **Strategic Trend Analysis Report: User Feedback (Last 48 Hours)**\n",
       "\n",
       "**To:** Leadership & Engineering Teams\n",
       "**From:** Head of Product\n",
       "**Date:** [Current Date]\n",
       "**Subject:** Analysis of Recent User Feedback & Immediate Action Plan\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Executive Summary\n",
       "\n",
       "While we see a high volume of generic positive feedback, our core service delivery is facing a crisis. Critical operational areas such as delivery, order accuracy, and customer support are failing, resulting in extremely low sentiment scores (averaging 1.1-1.6 out of 5) and creating significant user frustration. These issues are actively eroding user trust and require immediate, focused intervention to prevent churn and reputational damage.\n",
       "\n",
       "### 2. Critical Issues (Red Flags)\n",
       "\n",
       "Our analysis highlights three interconnected areas that are severely underperforming. These are not isolated complaints but patterns indicating systemic problems.\n",
       "\n",
       "*   **Core Fulfillment Failure (Delivery & Accuracy):**\n",
       "    *   **Data:** \"Delivery Delays\" is our second-highest specific complaint category (35 mentions) with a sentiment of 1.40/5. \"Order Accuracy/Missing Items\" follows closely with 21 mentions and a 1.67/5 sentiment.\n",
       "    *   **Analysis:** Our fundamental promise of delivering the correct items on time is broken. Verbatims mention \"constant delays,\" \"ETA that changes in every minute,\" and receiving \"expired and cheap quality products.\" The fact that even premium \"Swiggy Black\" members are experiencing this indicates a deep operational issue, not just an edge case. This is our most significant business risk.\n",
       "\n",
       "*   **Support System Breakdown:**\n",
       "    *   **Data:** \"Customer Support Issues\" has the lowest sentiment score across all topics (**1.14/5**) with a high volume of 28 mentions.\n",
       "    *   **Analysis:** Our support system is actively making bad situations worse. Users report that their issues are \"not resolved\" and describe the service as the \"worst.\" This means that when our core fulfillment fails, users have no effective recourse, turning a single bad experience into a reason to abandon the app entirely.\n",
       "\n",
       "*   **Trust & Transparency Gaps (Pricing & Payments):**\n",
       "    *   **Data:** \"Pricing & Fees\" has a critically low sentiment of 1.36/5. \"Payment Method Issues\" also shows problems, with verbatims citing un-refunded failed payments.\n",
       "    *   **Analysis:** Users feel cheated. Complaints about high delivery charges are compounded by reports of being \"charged more money after the order was placed.\" Furthermore, the failure to refund money from failed transactions is a cardinal sin in e-commerce, causing irreparable damage to user trust.\n",
       "\n",
       "### 3. Feature Requests & Feedback\n",
       "\n",
       "Beneath the frustration, users are telling us what they need to see improved:\n",
       "\n",
       "*   **Reliable Issue Resolution:** Users want a simple, in-app process to report missing, wrong, or expired items with photo/video evidence. The current system is perceived as a dead end.\n",
       "*   **Transparent Checkout:** Users are demanding clarity on costs. The checkout screen must explicitly break down all charges (item cost, delivery fee, taxes, etc.) *before* the payment is confirmed to eliminate surprises.\n",
       "*   **Accurate Delivery ETAs:** The fluctuating ETA is a major source of frustration. Users need a reliable and accurate real-time tracking experience.\n",
       "*   **Core App Stability:** Critical bugs, such as the \"minimum amount in cart page shows 1999\" and login problems, are blocking basic usability and must be fixed.\n",
       "\n",
       "### 4. Action Plan\n",
       "\n",
       "Based on this analysis, I am directing our teams to prioritize the following three initiatives immediately:\n",
       "\n",
       "1.  **Form a \"Fulfillment Task Force\" (Lead: Ops & Product):**\n",
       "    *   **Action:** Assemble a cross-functional team (Product, Ops, Eng) to map the entire order lifecycle from placement to delivery.\n",
       "    *   **Objective:** Identify the top 3 root causes of delays and inaccuracies within one week. Propose and begin implementing operational (e.g., picker training, inventory checks) and technical (e.g., inventory sync logic, ETA algorithm refinement) fixes in the next sprint.\n",
       "\n",
       "2.  **Prioritize a \"Self-Serve Resolution Flow\" (Lead: Product & Eng):**\n",
       "    *   **Action:** Immediately scope and begin development of an enhanced in-app \"Report an Issue\" feature.\n",
       "    *   **Objective:** In the next 2-3 sprints, deliver a flow where users can select specific items from an order, report a problem (missing, damaged, expired), upload photo evidence, and trigger an automated refund or credit. This will provide immediate resolution for users and reduce the load on our failing support channels.\n",
       "\n",
       "3.  **Launch a \"Trust & Stability\" Sprint (Lead: Eng & Design):**\n",
       "    *   **Action:** Dedicate the next engineering sprint to fixing critical financial and trust-related issues.\n",
       "    *   **Objective:** a) Resolve the \"payment failed, money deducted\" bug as the #1 priority. b) Fix the cart value and login bugs. c) Redesign the checkout UI to provide a crystal-clear, itemized cost breakdown before a user confirms their order."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Trend Analysis Report (1 Call - Gemini Pro)\n",
    "\n",
    "# Load Processed Data\n",
    "df = pd.read_csv(f\"{WORKING_DIR}/{APP_ID}_processed_reviews.csv\")\n",
    "\n",
    "# 1. Prepare Statistical Summary\n",
    "# We don't need to send 1000 reviews again. We send the aggregated data.\n",
    "topic_counts = df['topic'].value_counts().to_string()\n",
    "avg_rating_per_topic = df.groupby('topic')['score'].mean().round(2).to_string()\n",
    "\n",
    "# Get 3 representative examples per topic for context\n",
    "examples_text = \"\"\n",
    "for topic in df['topic'].unique():\n",
    "    samples = df[df['topic'] == topic]['content'].head(3).tolist()\n",
    "    examples_text += f\"\\nTopic: {topic}\\nExamples: {samples}\\n\"\n",
    "\n",
    "# 2. The Analysis Prompt\n",
    "analysis_prompt = f\"\"\"\n",
    "You are the Head of Product for this App. \n",
    "Based on the data below from the last {DAYS_TO_FETCH} days, write a Strategic Trend Analysis Report.\n",
    "\n",
    "Data provided:\n",
    "1. Topic Volume:\n",
    "{topic_counts}\n",
    "\n",
    "2. Average Sentiment (1-5 Stars) per Topic:\n",
    "{avg_rating_per_topic}\n",
    "\n",
    "3. Verbatim Examples:\n",
    "{examples_text}\n",
    "\n",
    "Output Requirements:\n",
    "1. **Executive Summary**: 2-3 sentences on the overall health.\n",
    "2. **Critical Issues (Red Flags)**: High volume, low rating areas. What is breaking?\n",
    "3. **Feature Requests & Feedback**: What do users want?\n",
    "4. **Action Plan**: 3 concrete steps for the engineering/product team.\n",
    "\n",
    "Output Format: Markdown.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üß† Calling Gemini 2.5 Pro for Strategic Analysis...\")\n",
    "\n",
    "try:\n",
    "    # Using Pro for better reasoning\n",
    "    model_pro = genai.GenerativeModel('gemini-2.5-pro')\n",
    "    \n",
    "    analysis_response = model_pro.generate_content(analysis_prompt)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    from IPython.display import Markdown\n",
    "    display(Markdown(analysis_response.text))\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Optional: Save Report\n",
    "    with open(f\"{WORKING_DIR}/{APP_ID}_report.md\", \"w\") as f:\n",
    "        f.write(analysis_response.text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Analysis Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:40:19.573417Z",
     "iopub.status.busy": "2026-01-07T08:40:19.572994Z",
     "iopub.status.idle": "2026-01-07T08:41:50.924072Z",
     "shell.execute_reply": "2026-01-07T08:41:50.923027Z",
     "shell.execute_reply.started": "2026-01-07T08:40:19.573384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 447 reviews.\n",
      "\n",
      "üîç PASS 1: Discovering Taxonomy...\n",
      "‚úÖ Taxonomy Locked: ['Overall App Experience & Satisfaction', 'Delivery Speed & Timeliness', 'Customer Support & Issue Resolution', 'Order Accuracy & Product Quality', 'Pricing & Delivery Charges', 'Payment Options & COD Availability', 'Uncategorized']\n",
      "\n",
      "üöÄ PASS 2: Classifying 447 reviews (Parallel Threads)...\n",
      "   ‚ö° Progress: 9/9 batches done...\n",
      "\n",
      "‚úÖ Done in 81.47 seconds!\n",
      "üìä Speed: 5.5 reviews/sec\n",
      "topic\n",
      "Overall App Experience & Satisfaction    293\n",
      "Delivery Speed & Timeliness               40\n",
      "Customer Support & Issue Resolution       38\n",
      "Order Accuracy & Product Quality          28\n",
      "Uncategorized                             21\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: High-Performance Parallel Classification\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv(f\"{WORKING_DIR}/{APP_ID}_raw_reviews.csv\")\n",
    "    print(f\"üìÇ Loaded {len(df)} reviews.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Run Cell 1 first to fetch data.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DISCOVERY_SAMPLE_SIZE = 100 \n",
    "BATCH_SIZE = 50           # Smaller batches are safer for parallel execution\n",
    "MAX_WORKERS = 8           # Number of simultaneous API calls (Parallel threads)\n",
    "\n",
    "model_flash = genai.GenerativeModel('gemini-2.5-flash')\n",
    "\n",
    "# ==========================================\n",
    "# PASS 1: TAXONOMY DISCOVERY (Serial - Fast)\n",
    "# ==========================================\n",
    "print(\"\\nüîç PASS 1: Discovering Taxonomy...\")\n",
    "TAXONOMY = []\n",
    "\n",
    "if not df.empty:\n",
    "    sample_reviews = df['content'].sample(min(DISCOVERY_SAMPLE_SIZE, len(df)), random_state=42).tolist()\n",
    "    sample_text = \"\\n\".join([f\"- {r[:150]}\" for r in sample_reviews])\n",
    "\n",
    "    discovery_prompt = f\"\"\"\n",
    "    Analyze these user reviews for a food delivery app.\n",
    "    Identify the Top 6 distinct categories.\n",
    "    Output ONLY a Python list of strings.\n",
    "    Reviews: {sample_text}\n",
    "    Example Output: [\"Category A\", \"Category B\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model_flash.generate_content(discovery_prompt)\n",
    "        cleaned_text = response.text.replace('```python', '').replace('```', '').replace('\\n', '').strip()\n",
    "        TAXONOMY = eval(cleaned_text)\n",
    "        if \"Uncategorized\" not in TAXONOMY: TAXONOMY.append(\"Uncategorized\")\n",
    "        print(f\"‚úÖ Taxonomy Locked: {TAXONOMY}\")\n",
    "    except:\n",
    "        TAXONOMY = [\"Delivery Issues\", \"App Bugs\", \"Food Quality\", \"Refunds\", \"Positive Feedback\", \"Uncategorized\"]\n",
    "        print(\"‚ö†Ô∏è Used Default Taxonomy.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # PASS 2: PARALLEL BATCH CLASSIFICATION\n",
    "    # ==========================================\n",
    "    print(f\"\\nüöÄ PASS 2: Classifying {len(df)} reviews (Parallel Threads)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Define the Worker Function\n",
    "    def process_batch(batch_df, batch_index):\n",
    "        batch_text = \"\"\n",
    "        for _, row in batch_df.iterrows():\n",
    "            # Clean and minify text\n",
    "            clean_text = str(row['content']).replace('\\n', ' ').replace('\"', \"'\")[:200]\n",
    "            batch_text += f\"{row['id']}: {clean_text}\\n\"\n",
    "            \n",
    "        prompt = f\"\"\"\n",
    "        Map each Review ID to exactly ONE category from: {TAXONOMY}\n",
    "        Output strict JSON: {{\"id\": \"category\"}}\n",
    "        Reviews:\n",
    "        {batch_text}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Add retries for stability\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    resp = model_flash.generate_content(\n",
    "                        prompt, \n",
    "                        generation_config={\"response_mime_type\": \"application/json\"}\n",
    "                    )\n",
    "                    return json.loads(resp.text)\n",
    "                except Exception as e:\n",
    "                    if \"429\" in str(e): # Rate limit hit\n",
    "                        time.sleep(2 * (attempt + 1)) # Backoff\n",
    "                        continue\n",
    "                    raise e\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Batch {batch_index} Failed: {e}\")\n",
    "            return {rid: \"Uncategorized\" for rid in batch_df['id']}\n",
    "\n",
    "    # 2. Split Data into Chunks\n",
    "    batches = []\n",
    "    num_batches = math.ceil(len(df) / BATCH_SIZE)\n",
    "    for i in range(num_batches):\n",
    "        batches.append((df.iloc[i*BATCH_SIZE : (i+1)*BATCH_SIZE], i))\n",
    "\n",
    "    # 3. Execute in Parallel\n",
    "    review_category_map = {}\n",
    "    completed_count = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_batch = {executor.submit(process_batch, b_df, idx): idx for b_df, idx in batches}\n",
    "        \n",
    "        # Process as they complete\n",
    "        for future in as_completed(future_to_batch):\n",
    "            batch_result = future.result()\n",
    "            review_category_map.update(batch_result)\n",
    "            completed_count += 1\n",
    "            print(f\"\\r   ‚ö° Progress: {completed_count}/{num_batches} batches done...\", end=\"\")\n",
    "\n",
    "    # ==========================================\n",
    "    # MERGE & SAVE\n",
    "    # ==========================================\n",
    "    df['topic'] = df['id'].map(review_category_map).fillna(\"Uncategorized\")\n",
    "    \n",
    "    processed_filename = f\"{WORKING_DIR}/{APP_ID}_processed_reviews.csv\"\n",
    "    df.to_csv(processed_filename, index=False)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n\\n‚úÖ Done in {elapsed:.2f} seconds!\")\n",
    "    print(f\"üìä Speed: {len(df) / elapsed:.1f} reviews/sec\")\n",
    "    print(df['topic'].value_counts().head())\n",
    "\n",
    "else:\n",
    "    print(\"No data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
